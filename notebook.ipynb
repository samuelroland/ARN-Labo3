{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labo 3 - ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mice_eeg = pd.read_csv('./pw3_data/EEG_mouse_data_1.csv')\n",
    "mice_eeg2 = pd.read_csv('./pw3_data/EEG_mouse_data_2.csv')\n",
    "df = pd.concat([mice_eeg, mice_eeg2], ignore_index=True)\n",
    "\n",
    "mice_eeg3 = pd.read_csv('./pw3_data/EEG_mouse_data_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the first 25 features as they are contain the biggest occurences count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = df.iloc[:, 1:26].to_numpy()\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut in 3 folds (1/3 for test and 2/3 for training)\n",
    "# shuffle d'abord avant de dÃ©couper\n",
    "#  normaliser tout fit\n",
    "# partager\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "pl.clf()\n",
    "\n",
    "keras.utils.set_random_seed(123)\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(df)):\n",
    "    print(str(i) + ' ' + str(train_index) + ' ' + str(test_index))\n",
    "# See training 2 cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and setup our MLP\n",
    "# Source: From the notebook 5. See 'Changed: '\n",
    "def create_model():\n",
    "  mlp = keras.Sequential([\n",
    "      layers.Input(shape=(25,)), # Changed: shape=(25,) instead of 2\n",
    "      layers.Dense(8, activation=\"tanh\"), # Try different numbers of hidden neurons here (e.g. 2, 4, 8, 32, 128)\n",
    "      # Changed: todo try to change the amount of layers and perceptrons per layer\n",
    "      layers.Dense(1, activation=\"tanh\"),   # next \n",
    "  ])\n",
    "\n",
    "  # Experiment with hyperparameters here:\n",
    "  # momentum: [0, 0.8, 0.9, 0.99]\n",
    "  # learning_rate: [0.1, 0.01, 0.001, 0.0001]\n",
    "  mlp.compile(\n",
    "      optimizer=keras.optimizers.SGD(learning_rate=0.001, momentum=0.99),\n",
    "      loss=\"mse\",\n",
    "  )\n",
    "\n",
    "  return mlp\n",
    "\n",
    "mlp = create_model()\n",
    "mlp.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with first column \"state\" and replace n with 1 and other values with -1, \n",
    "# convert it as numpy array to access rows and not columns\n",
    "output_data = df.iloc[:,0].map(lambda e: 1 if e == 'n' else -1).to_numpy()\n",
    "output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training on 3 folds\n",
    "history_list = []\n",
    "trained_mlp = []\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(output_data.shape)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(input_data)):\n",
    "  print(str(i) + ' ' + str(train_index) + ' ' + str(test_index))\n",
    "  \n",
    "  # We need to create a new model everytime otherwise fit will continue previous training\n",
    "  mlp = create_model()\n",
    "  \n",
    "  train_input = scaler.fit_transform(input_data[train_index])\n",
    "  test_input = scaler.fit_transform(input_data[test_index])\n",
    "  \n",
    "  history = mlp.fit(\n",
    "      x=train_input, \n",
    "      y=output_data[train_index],\n",
    "      validation_data=(test_input, output_data[test_index]),\n",
    "      epochs=4\n",
    "  )\n",
    "\n",
    "  history_list.append(history)\n",
    "  trained_mlp.append(mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_mlp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_losses = np.array([history.history['loss'] for history in history_list])\n",
    "val_losses = np.array([history.history['val_loss'] for history in history_list])\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation losses\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "std_train_loss = np.std(train_losses, axis=0)\n",
    "mean_val_loss = np.mean(val_losses, axis=0)\n",
    "std_val_loss = np.std(val_losses, axis=0)\n",
    "\n",
    "# Plot mean and standard deviation for training loss\n",
    "pl.plot(mean_train_loss, label='Training Loss (Mean)')\n",
    "pl.fill_between(range(len(mean_train_loss)), mean_train_loss - std_train_loss, mean_train_loss + std_train_loss, alpha=0.3, label='Training Loss (Std)')\n",
    "\n",
    "# Plot mean and standard deviation for validation loss\n",
    "pl.plot(mean_val_loss, label='Validation Loss (Mean)')\n",
    "pl.fill_between(range(len(mean_val_loss)), mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, alpha=0.3, label='Validation Loss (Std)')\n",
    "\n",
    "# Add labels and legend\n",
    "pl.xlabel('Epochs')\n",
    "pl.ylabel('Loss')\n",
    "pl.legend()\n",
    "\n",
    "# Display the plot\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, title):\n",
    "    # Plot confusion matrix\n",
    "    pl.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix.astype(int), annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"Blue\", \"Red\"], yticklabels=[\"Blue\", \"Red\"])\n",
    "    pl.title(title)\n",
    "    pl.xlabel('Predicted')\n",
    "    pl.ylabel('True')\n",
    "    pl.show()\n",
    "\n",
    "f1_scores = []\n",
    "mean_confusion_matrix = np.zeros((2, 2))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(input_data)):\n",
    "    \n",
    "    # Normalize\n",
    "    test_input = scaler.fit_transform(input_data[test_index])\n",
    "\n",
    "    # Evaluate the trained model on the test fold\n",
    "    predictions = (trained_mlp[i].predict(test_input) > 0).astype(int)\n",
    "    true_labels = (output_data[test_index] > 0).astype(int)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    mean_confusion_matrix += confusion_matrix(true_labels, predictions)\n",
    "\n",
    "    # Compute confusion matrix and plot\n",
    "    plot_confusion_matrix(cm, f'Confusion Matrix - Fold {i + 1}')\n",
    "\n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"F1 Score - Fold {i + 1}: {f1}\")\n",
    "\n",
    "# Plot mean confusion matrix\n",
    "plot_confusion_matrix(mean_confusion_matrix, 'Global confusion matrix')\n",
    "\n",
    "# Calculate and display the mean F1 score across all folds\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "print(f\"Mean F1 Score across all folds: {mean_f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note for later: next section will use 3 instead of 1 because 3 classes, and ~ supermax \n",
    "layers.Dense(1, activation=\"tanh\"),   # next \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
