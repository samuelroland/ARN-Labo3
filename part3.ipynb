{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labo 3 - ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:41:06.856433Z",
     "start_time": "2024-04-19T20:41:04.698725Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mice_eeg = pd.read_csv('./pw3_data/EEG_mouse_data_1.csv')\n",
    "mice_eeg2 = pd.read_csv('./pw3_data/EEG_mouse_data_2.csv')\n",
    "df = pd.concat([mice_eeg, mice_eeg2], ignore_index=True)\n",
    "\n",
    "#mice_eeg3 = pd.read_csv('./pw3_data/EEG_mouse_data_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the first 25 features as they are contain the biggest occurences count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:41:06.868783Z",
     "start_time": "2024-04-19T20:41:06.859428Z"
    }
   },
   "outputs": [],
   "source": [
    "input_data = df.iloc[:, 1:26].to_numpy()\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:41:06.906192Z",
     "start_time": "2024-04-19T20:41:06.870721Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cut in 3 folds (1/3 for test and 2/3 for training)\n",
    "# shuffle d'abord avant de d√©couper\n",
    "#  normaliser tout fit\n",
    "# partager\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "pl.clf()\n",
    "\n",
    "keras.utils.set_random_seed(123)\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(df)):\n",
    "    print(str(i) + ' ' + str(train_index) + ' ' + str(test_index))\n",
    "# See training 2 cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:41:07.011978Z",
     "start_time": "2024-04-19T20:41:06.910363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure and setup our MLP\n",
    "# Source: From the notebook 5. See 'Changed: '\n",
    "def create_model():\n",
    "  mlp = keras.Sequential([\n",
    "      layers.Input(shape=(25,)), # Changed: shape=(25,) instead of 2\n",
    "      layers.Dense(20, activation=\"leaky_relu\"), # Try different numbers of hidden neurons here (e.g. 2, 4, 8, 32, 128)\n",
    "      # Changed: todo try to change the amount of layers and perceptrons per layer\n",
    "      layers.Dense(3, activation=\"softmax\")   # output\n",
    "  ])\n",
    "\n",
    "  # Experiment with hyperparameters here:\n",
    "  # momentum: [0, 0.8, 0.9, 0.99]\n",
    "  # learning_rate: [0.1, 0.01, 0.001, 0.0001]\n",
    "  mlp.compile(\n",
    "      optimizer=\"rmsprop\",\n",
    "      loss=keras.losses.CategoricalFocalCrossentropy(),\n",
    "  )\n",
    "\n",
    "  return mlp\n",
    "\n",
    "mlp = create_model()\n",
    "mlp.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:41:07.017783Z",
     "start_time": "2024-04-19T20:41:07.014415Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:48:12.150871Z",
     "start_time": "2024-04-19T20:41:07.019957Z"
    }
   },
   "outputs": [],
   "source": [
    "history_list = []\n",
    "trained_mlp = []\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "scaler = StandardScaler()\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_states = encoder.fit_transform(pd.DataFrame(df['state']))  # Transform and encode the states\n",
    "\n",
    "print(encoded_states)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(input_data)):\n",
    "  print(str(i) + ' ' + str(train_index) + ' ' + str(test_index))\n",
    "  \n",
    "  # We need to create a new model every time otherwise fit will continue previous training\n",
    "  mlp = create_model()\n",
    "  \n",
    "  train_input = scaler.fit_transform(input_data[train_index])\n",
    "  test_input = scaler.fit_transform(input_data[test_index])\n",
    "  \n",
    "  history = mlp.fit(\n",
    "      x=train_input, \n",
    "      y=encoded_states[train_index],  # Use the encoded states for training\n",
    "      validation_data=(test_input, encoded_states[test_index]),\n",
    "      epochs=100\n",
    "  )\n",
    "\n",
    "  history_list.append(history)\n",
    "  trained_mlp.append(mlp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:48:12.157918Z",
     "start_time": "2024-04-19T20:48:12.152922Z"
    }
   },
   "outputs": [],
   "source": [
    "trained_mlp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:48:12.585160Z",
     "start_time": "2024-04-19T20:48:12.160243Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_losses = np.array([history.history['loss'] for history in history_list])\n",
    "val_losses = np.array([history.history['val_loss'] for history in history_list])\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation losses\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "std_train_loss = np.std(train_losses, axis=0)\n",
    "mean_val_loss = np.mean(val_losses, axis=0)\n",
    "std_val_loss = np.std(val_losses, axis=0)\n",
    "\n",
    "# Plot mean and standard deviation for training loss\n",
    "pl.plot(mean_train_loss, label='Training Loss (Mean)')\n",
    "pl.fill_between(range(len(mean_train_loss)), mean_train_loss - std_train_loss, mean_train_loss + std_train_loss, alpha=0.3, label='Training Loss (Std)')\n",
    "\n",
    "# Plot mean and standard deviation for validation loss\n",
    "pl.plot(mean_val_loss, label='Validation Loss (Mean)')\n",
    "pl.fill_between(range(len(mean_val_loss)), mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, alpha=0.3, label='Validation Loss (Std)')\n",
    "\n",
    "# Add labels and legend\n",
    "pl.xlabel('Epochs')\n",
    "pl.ylabel('Loss')\n",
    "pl.legend()\n",
    "\n",
    "# Display the plot\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T20:48:16.629198Z",
     "start_time": "2024-04-19T20:48:12.586767Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, title):\n",
    "    # Plot confusion matrix\n",
    "    pl.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix.astype(int), annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"rem\",\"n-rem\",\"awake\"], yticklabels=[\"rem\",\"n-rem\", \"awake\"])\n",
    "    pl.title(title)\n",
    "    pl.xlabel('Predicted')\n",
    "    pl.ylabel('True')\n",
    "    pl.show()\n",
    "\n",
    "f1_scores = []\n",
    "mean_confusion_matrix = np.zeros((3, 3))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(input_data)):\n",
    "    \n",
    "    # Normalize\n",
    "    train_input = scaler.fit_transform(input_data[train_index])\n",
    "    test_input = scaler.transform(input_data[test_index])\n",
    "\n",
    "    # Transform target variable\n",
    "    train_output = encoder.transform(df['state'].iloc[train_index].values.reshape(-1, 1))\n",
    "    test_output = encoder.transform(df['state'].iloc[test_index].values.reshape(-1, 1))\n",
    "    \n",
    "    # Get feature names from encoder categories\n",
    "    feature_names = encoder.get_feature_names_out(['state'])\n",
    "    train_output_df = pd.DataFrame(train_output, columns=feature_names)\n",
    "    test_output_df = pd.DataFrame(test_output, columns=feature_names)\n",
    "\n",
    "    # Evaluate the trained model on the test fold\n",
    "    predictions = trained_mlp[i].predict(test_input)\n",
    "    true_labels = np.argmax(test_output, axis=1)  # Get the index of the maximum value as the true labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1)  # Get the index of the maximum value as the predicted labels\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    mean_confusion_matrix += cm\n",
    "\n",
    "    # Compute confusion matrix and plot\n",
    "    plot_confusion_matrix(cm, f'Confusion Matrix - Fold {i + 1}')\n",
    "\n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='macro')  # Use macro averaging for multi-class classification\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"F1 Score - Fold {i + 1}: {f1}\")\n",
    "\n",
    "# Plot mean confusion matrix\n",
    "plot_confusion_matrix(mean_confusion_matrix, 'Global confusion matrix')\n",
    "\n",
    "# Calculate and display the mean F1 score across all folds\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "print(f\"Mean F1 Score across all folds: {mean_f1_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data predictions\n",
    "Running the best of our 3 models on test data for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = pd.read_csv('./pw3_data/EEG_mouse_data_test.csv')\n",
    "\n",
    "# Take the first 25 columns only\n",
    "testdf = testdf.iloc[:, 0:25]\n",
    "# Normalize test set (to have the same kind of inputs on the model)\n",
    "test_data = scaler.transform(testdf.to_numpy()) \n",
    "\n",
    "# Analysis\n",
    "# train_output = encoder.transform(df['state'].iloc[train_index].values.reshape(-1, 1))\n",
    "# print(df['state'])\n",
    "# print(train_output)\n",
    "# Note: when inspecting this output, I deduced that n is at index 0, r at index 1 and w at index 0\n",
    "\n",
    "# Takethe best model based on the maximum f1_score\n",
    "best_model_index = f1_scores.index(max(f1_scores))\n",
    "print(\"best_model_index \", best_model_index)\n",
    "\n",
    "# Run predictions on test data\n",
    "print(\"Running predictions on test data\")\n",
    "test_predictions = trained_mlp[best_model_index].predict(test_data)\n",
    "\n",
    "# Transform predictions intos labels index and then labels\n",
    "print(test_predictions)\n",
    "predicted_labels = np.argmax(test_predictions, axis=1)  # Get the index of the maximum value as the predicted labels\n",
    "print(predicted_labels)\n",
    "labels = ['n', 'r', 'w'] # because n is at index 0, r at index 1 and w at index 0\n",
    "predicted_labels_state = [labels[i] for i in predicted_labels]\n",
    "# Printing 20 values in the middle to check for state coherence\n",
    "print(predicted_labels[1500:1520])\n",
    "print(predicted_labels_state[1500:1520])\n",
    "\n",
    "# Write predictions nparray to file\n",
    "with open('test_pred.npy', 'wb') as f:\n",
    "    np.save(f, predicted_labels_state)\n",
    "\n",
    "print(\"file test_pred.npy saved !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
